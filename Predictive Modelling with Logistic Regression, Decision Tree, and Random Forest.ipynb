{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:/Users/CampusUser/Anaconda3/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a predictive model on the data set, \n",
    "Skicit-Learn (sklearn) is the most commonly used library in Python for this purpose and sklearn requires all inputs to be numeric.\n",
    "So before converting all our categorical variables into numeric by encoding the categories, let's take care of the missing values in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
    "df['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               object\n",
       "Gender                 int64\n",
       "Married                int64\n",
       "Dependents             int64\n",
       "Education              int64\n",
       "Self_Employed          int64\n",
       "ApplicantIncome        int64\n",
       "CoapplicantIncome    float64\n",
       "LoanAmount           float64\n",
       "Loan_Amount_Term     float64\n",
       "Credit_History       float64\n",
       "Property_Area          int64\n",
       "Loan_Status            int64\n",
       "Unnamed: 13          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required modules, we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CampusUser\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\CampusUser\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Importing models from scikit learn module:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold   #For K-fold cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import models from scikit learn module:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold   #For K-fold cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "\n",
    "#Generic function for making a classification model and accessing performance:\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "  #Fit the model:\n",
    "  model.fit(data[predictors],data[outcome])\n",
    "  \n",
    "  #Make predictions on training set:\n",
    "  predictions = model.predict(data[predictors])\n",
    "  \n",
    "  #Print accuracy\n",
    "  accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "  print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "  #Perform k-fold cross-validation with 5 folds\n",
    "  kf = KFold(data.shape[0], n_folds=5)\n",
    "  error = []\n",
    "  for train, test in kf:\n",
    "    # Filter training data\n",
    "    train_predictors = (data[predictors].iloc[train,:])\n",
    "    \n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = data[outcome].iloc[train]\n",
    "    \n",
    "    # Training the algorithm using the predictors and target.\n",
    "    model.fit(train_predictors, train_target)\n",
    "    \n",
    "    #Record error from each cross-validation run\n",
    "    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    " \n",
    "  print (\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n",
    "\n",
    "  #Fit the model again so that it can be refered outside the function:\n",
    "  model.fit(data[predictors],data[outcome]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: \n",
    "We can easily make some intuitive hypothesis to set the ball rolling. \n",
    "The chances of getting a loan will be higher for:\n",
    "* Applicants having a credit history (remember we observed this in exploration?)\n",
    "* Applicants with higher applicant and co-applicant incomes\n",
    "* Applicants with higher education level\n",
    "* Properties in urban areas with high growth perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 80.945%\n",
      "Cross-Validation Score : 80.942%\n"
     ]
    }
   ],
   "source": [
    "out_var = 'Loan_Status'\n",
    "model = LogisticRegression()\n",
    "pred_var = ['Credit_History']\n",
    "classification_model(model, df,pred_var,out_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 80.945%\n",
      "Cross-Validation Score : 80.942%\n"
     ]
    }
   ],
   "source": [
    "#We can try different combination of variables:\n",
    "pred_var = ['Credit_History','Education','Married','Self_Employed','Property_Area']\n",
    "classification_model(model, df,pred_var,out_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the accuracy is expected to increase on adding variables. But, here the accuracy and cross-validation score are not getting impacted by less important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 80.945%\n",
      "Cross-Validation Score : 80.942%\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree: It is known to provide higher accuracy than logistic regression model.\n",
    "model = DecisionTreeClassifier()\n",
    "pred_var = ['Credit_History','Gender','Married','Education']\n",
    "classification_model(model, df,pred_var,out_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model based on categorical variables is unable to have an impact because Credit History is dominating over them\n",
    "df['LoanAmount_log'] = np.log(df['LoanAmount'])\n",
    "df['LoanAmount_log'].fillna(df['LoanAmount_log'].mean())\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "df['TotalIncome_log'] = np.log(df['TotalIncome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 81.270%\n",
      "Cross-Validation Score : 79.964%\n"
     ]
    }
   ],
   "source": [
    "#We can try different combination of variables:\n",
    "pred_var = ['Credit_History','Loan_Amount_Term','LoanAmount_log']\n",
    "classification_model(model, df,pred_var,out_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:\n",
    "Random forest is another algorithm for solving the classification problem.\n",
    "An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.837%\n",
      "Cross-Validation Score : 76.218%\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "predictor_var = ['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n",
    "        'LoanAmount_log','TotalIncome_log']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalIncome_log     0.448355\n",
      "Credit_History      0.280170\n",
      "Dependents          0.064225\n",
      "Loan_Amount_Term    0.055405\n",
      "Property_Area       0.051871\n",
      "Education           0.027904\n",
      "Gender              0.025259\n",
      "Self_Employed       0.023764\n",
      "Married             0.023046\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Create a series with feature importances:\n",
    "featimp = pd.Series(model.feature_importances_, index=pred_var).sort_values(ascending=False)\n",
    "print (featimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 82.410%\n",
      "Cross-Validation Score : 80.293%\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=25, min_samples_split=25, max_depth=7, max_features=1)\n",
    "pred_var = ['TotalIncome_log','LoanAmount_log','Credit_History','Dependents','Property_Area']\n",
    "classification_model(model, df,pred_var,out_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although accuracy reduced, but the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. we have reached a cross-validation accuracy only slightly better than the original logistic regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
